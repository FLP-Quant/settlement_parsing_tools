{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf5a0dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import keyring\n",
    "from datetime import datetime\n",
    "\n",
    "# Get the parent directory of the notebook (i.e., project root)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from src.automated_isone_data_update import automated_isone_data_update\n",
    "\n",
    "# FLP database connection tools path\n",
    "flp_db_tools_path = r\"C:\\Users\\cbrooks\\OneDrive - FIRSTLIGHTPOWER.COM\\Documents\\Python\\flp_database_connection_tools\"\n",
    "database_helpers = os.path.join(flp_db_tools_path,\"Helpers\")\n",
    "if database_helpers not in sys.path:\n",
    "    sys.path.append(database_helpers)\n",
    "from flp_database_connector import flp_database_connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61c394f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inputs\n",
    "# Get token from Windows Credential Manager\n",
    "token = keyring.get_password(\"pharos_api\", \"api_token\")\n",
    "USERNAME = r\"firstlightpower\\cbrooks\"\n",
    "tz = 'America/New_York'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b02d52bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using user-specified start_date: 2025-03-01\n",
      "Checking for missing data from 2025-03-01 to 2026-01-06\n",
      "Found 34216 missing records across 35 dates\n",
      "Missing dates: [datetime.date(2025, 3, 9), datetime.date(2025, 6, 28), datetime.date(2025, 6, 29), datetime.date(2025, 6, 30), datetime.date(2025, 7, 1), datetime.date(2025, 7, 2), datetime.date(2025, 7, 5), datetime.date(2025, 7, 6), datetime.date(2025, 7, 7), datetime.date(2025, 7, 8)]...\n",
      "Grouped into 6 contiguous date ranges\n",
      "Querying API 1/6: 2025-03-09 to 2025-03-10\n",
      "  Retrieved 61 rows\n",
      "Querying API 2/6: 2025-06-28 to 2025-07-03\n",
      "  Retrieved 66 rows\n",
      "Querying API 3/6: 2025-07-05 to 2025-07-10\n",
      "  Retrieved 66 rows\n",
      "Querying API 4/6: 2025-07-16 to 2025-07-17\n",
      "  Retrieved 110 rows\n",
      "Querying API 5/6: 2025-07-24 to 2025-07-26\n",
      "  Retrieved 45 rows\n",
      "Querying API 6/6: 2025-12-18 to 2026-01-08\n",
      "  Retrieved 1566 rows\n",
      "\n",
      "Combining data from 6 API responses...\n",
      "Processing combined data...\n",
      "WARNING: DataFrame 1 has 40 columns, expected 13\n",
      "First few actual column names: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "First row sample: {0: 'FirstLight Power (50876)', 1: 'SD_DAASCLEARED', 2: '2025-03-10', 3: '2025-07-13 17:07:51 UTC', 4: 'C', 5: 'Firstlight Power Management LL', 6: None, 7: None, 8: None, 9: None}\n",
      "Using only the first 13 columns and ignoring the rest.\n",
      "WARNING: DataFrame 2 has 40 columns, expected 13\n",
      "First few actual column names: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "First row sample: {0: 'FirstLight Power (50876)', 1: 'SD_DAASCLEARED', 2: '2025-07-03', 3: '2025-11-16 16:06:28 UTC', 4: 'C', 5: 'Firstlight Power Management LL', 6: None, 7: None, 8: None, 9: None}\n",
      "Using only the first 13 columns and ignoring the rest.\n",
      "WARNING: DataFrame 3 has 40 columns, expected 13\n",
      "First few actual column names: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "First row sample: {0: 'FirstLight Power (50876)', 1: 'SD_DAASCLEARED', 2: '2025-07-10', 3: '2025-11-16 16:14:27 UTC', 4: 'C', 5: 'Firstlight Power Management LL', 6: None, 7: None, 8: None, 9: None}\n",
      "Using only the first 13 columns and ignoring the rest.\n",
      "WARNING: DataFrame 4 has 40 columns, expected 13\n",
      "First few actual column names: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "First row sample: {0: 'FirstLight Power (50876)', 1: 'SD_DAASCLEARED', 2: '2025-07-17', 3: '2025-11-16 16:24:58 UTC', 4: 'C', 5: 'Firstlight Power Management LL', 6: None, 7: None, 8: None, 9: None}\n",
      "Using only the first 13 columns and ignoring the rest.\n",
      "WARNING: DataFrame 5 has 40 columns, expected 13\n",
      "First few actual column names: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "First row sample: {0: 'FirstLight Power (50876)', 1: 'SD_DAASCLEARED', 2: '2025-07-26', 3: '2025-11-16 16:35:34 UTC', 4: 'C', 5: 'Firstlight Power Management LL', 6: None, 7: None, 8: None, 9: None}\n",
      "Using only the first 13 columns and ignoring the rest.\n",
      "WARNING: DataFrame 6 has 40 columns, expected 13\n",
      "First few actual column names: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "First row sample: {0: 'FirstLight Power (50876)', 1: 'SD_DAASCLEARED', 2: '2026-01-04', 3: '2026-01-07 06:09:32 UTC', 4: 'C', 5: 'Firstlight Power Management LL', 6: None, 7: None, 8: None, 9: None}\n",
      "Using only the first 13 columns and ignoring the rest.\n",
      "0 duplicate rows removed based on most recent Version.\n",
      "Processed 6620 rows for upload.\n",
      "\n",
      "Deduplicating against existing database records...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cbrooks\\OneDrive - FIRSTLIGHTPOWER.COM\\Documents\\Python\\settlement_parsing_tools\\src\\automated_isone_data_update.py:546: UserWarning: WARNING: Found 207 records where existing database value was 0, but API returned nonzero data. These will be updated.\n",
      "Sample records being overwritten:\n",
      "                 datetime_he                   name    ops_type service da_volume\n",
      "4  2025-03-09 04:00:00-04:00  NORTHFIELD MOUNTAIN 4  Generation     EIR         0\n",
      "5  2025-03-09 04:00:00-04:00  NORTHFIELD MOUNTAIN 4  Generation   TMNSR         0\n",
      "6  2025-03-09 04:00:00-04:00  NORTHFIELD MOUNTAIN 4  Generation    TMSR         0\n",
      "7  2025-03-09 05:00:00-04:00  NORTHFIELD MOUNTAIN 4  Generation     EIR         0\n",
      "8  2025-03-09 05:00:00-04:00  NORTHFIELD MOUNTAIN 4  Generation    TMOR         0\n",
      "9  2025-03-09 05:00:00-04:00  NORTHFIELD MOUNTAIN 4  Generation    TMSR         0\n",
      "10 2025-03-09 06:00:00-04:00  NORTHFIELD MOUNTAIN 4  Generation     EIR         0\n",
      "11 2025-03-09 06:00:00-04:00  NORTHFIELD MOUNTAIN 4  Generation    TMOR         0\n",
      "12 2025-03-09 06:00:00-04:00  NORTHFIELD MOUNTAIN 4  Generation    TMSR         0\n",
      "13 2025-03-09 07:00:00-04:00  NORTHFIELD MOUNTAIN 4  Generation     EIR         0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After deduplication: 6601 rows remaining for upload.\n",
      "\n",
      "Checking for any remaining missing data to fill with zeros...\n",
      "Data range: 2025-03-09 01:00:00-05:00 to 2026-01-05 00:00:00-05:00\n",
      "\n",
      "Filling 312315 missing combinations with default values (0) for gaps between 2025-03-09 01:00:00-05:00 and 2026-01-05 00:00:00-05:00...\n",
      "Added 312315 records with default values.\n",
      "\n",
      "Performing final deduplication check before upload...\n",
      "No duplicates found in final data (count: 318916)\n",
      "Uploading to database...\n",
      "Target timezone for ops.isone_hourly_ancillary: America/New_York\n",
      "Only datetime_he column was provided. datetime_hb column was created...\n",
      "Beginning data upload of 318916 rows in update mode...\n",
      "Data uploaded to database. Beginning merge with updates...\n",
      "Update complete! Updated/inserted 318916 rows into ops.isone_hourly_ancillary\n",
      "Upload complete!\n"
     ]
    }
   ],
   "source": [
    "# Automatically refresh data in DAAS positions database\n",
    "db_table_name = \"ops.isone_hourly_ancillary\"\n",
    "mis_report = \"SD_DAASCLEARED\"\n",
    "automated_isone_data_update(USERNAME, token, db_table_name, tz, mis_report, datetime(2025,3,1),fill_with_zeros=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2898a579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for missing data from 2025-03-01 to 2025-12-20\n",
      "Found 233640 missing records across 296 dates\n",
      "Missing dates: [datetime.date(2025, 3, 1), datetime.date(2025, 3, 2), datetime.date(2025, 3, 3), datetime.date(2025, 3, 4), datetime.date(2025, 3, 5), datetime.date(2025, 3, 6), datetime.date(2025, 3, 7), datetime.date(2025, 3, 8), datetime.date(2025, 3, 9), datetime.date(2025, 3, 10)]...\n",
      "Segmented into 10 monthly chunks (max 30 days each) for OI_UNITRTRSV\n",
      "Grouped into 10 contiguous date ranges\n",
      "Querying API 1/10: 2025-03-01 to 2025-03-31\n",
      "  Retrieved 156734 rows\n",
      "Querying API 2/10: 2025-03-31 to 2025-04-30\n",
      "  Retrieved 156928 rows\n",
      "Querying API 3/10: 2025-04-30 to 2025-05-30\n",
      "  Retrieved 156707 rows\n",
      "Querying API 4/10: 2025-05-30 to 2025-06-29\n",
      "  Retrieved 156751 rows\n",
      "Querying API 5/10: 2025-06-29 to 2025-07-29\n",
      "  Retrieved 156962 rows\n",
      "Querying API 6/10: 2025-07-29 to 2025-08-28\n",
      "  Retrieved 156979 rows\n",
      "Querying API 7/10: 2025-08-28 to 2025-09-27\n",
      "  Retrieved 156673 rows\n",
      "Querying API 8/10: 2025-09-27 to 2025-10-27\n",
      "  Retrieved 156962 rows\n",
      "Querying API 9/10: 2025-10-27 to 2025-11-26\n",
      "  Retrieved 156809 rows\n",
      "Querying API 10/10: 2025-11-26 to 2025-12-22\n",
      "  Retrieved 133325 rows\n",
      "\n",
      "Combining data from 10 API responses...\n",
      "Processing combined data...\n",
      "1488481 duplicate rows removed based on most recent Version.\n",
      "Processed 1296 rows for upload.\n",
      "\n",
      "Deduplicating against existing database records...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cbrooks\\OneDrive - FIRSTLIGHTPOWER.COM\\Documents\\Python\\settlement_parsing_tools\\src\\process_as_positions.py:373: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After deduplication: 1440 rows remaining for upload.\n",
      "\n",
      "Checking for any remaining missing data...\n",
      "Uploading to database...\n",
      "Target timezone for ops.isone_hourly_ancillary: America/New_York\n",
      "Only datetime_he column was provided. datetime_hb column was created...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cbrooks\\OneDrive - FIRSTLIGHTPOWER.COM\\Documents\\Python\\settlement_parsing_tools\\src\\automated_isone_data_update.py:541: UserWarning: WARNING: API returned no data for 296 dates. These dates will NOT have any records uploaded.\n",
      "Sample dates: [datetime.date(2025, 3, 1), datetime.date(2025, 3, 2), datetime.date(2025, 3, 3), datetime.date(2025, 3, 4), datetime.date(2025, 3, 5), datetime.date(2025, 3, 6), datetime.date(2025, 3, 7), datetime.date(2025, 3, 8), datetime.date(2025, 3, 9), datetime.date(2025, 3, 10)]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning data upload of 1440 rows in update mode...\n",
      "Data uploaded to database. Beginning merge with updates...\n",
      "Update complete! Updated/inserted 1440 rows into ops.isone_hourly_ancillary\n",
      "Upload complete!\n"
     ]
    }
   ],
   "source": [
    "# Automatically refresh data in RT reserves positions database\n",
    "db_table_name = \"ops.isone_hourly_ancillary\"\n",
    "mis_report = \"OI_UNITRTRSV\"\n",
    "automated_isone_data_update(USERNAME, token, db_table_name, tz, mis_report, datetime(2025,3,1), fill_with_zeros=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9dc1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using user-specified start_date: 2025-01-01\n",
      "Checking for missing data from 2025-01-01 to 2026-01-05\n",
      "Found 60020 missing records across 126 dates\n",
      "Missing dates: [datetime.date(2025, 9, 3), datetime.date(2025, 9, 4), datetime.date(2025, 9, 5), datetime.date(2025, 9, 6), datetime.date(2025, 9, 7), datetime.date(2025, 9, 8), datetime.date(2025, 9, 9), datetime.date(2025, 9, 10), datetime.date(2025, 9, 11), datetime.date(2025, 9, 12)]...\n",
      "Grouped into 1 contiguous date ranges\n",
      "Querying API 1/1: 2025-09-03 to 2026-01-07\n",
      "  Retrieved 67106 rows\n",
      "\n",
      "Combining data from 1 API responses...\n",
      "Processing combined data...\n",
      "No duplicate rows found.\n",
      "Processed 59540 rows for upload.\n",
      "\n",
      "Deduplicating against existing database records...\n",
      "After deduplication: 59540 rows remaining for upload.\n",
      "\n",
      "Checking for any remaining missing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cbrooks\\OneDrive - FIRSTLIGHTPOWER.COM\\Documents\\Python\\settlement_parsing_tools\\src\\automated_isone_data_update.py:652: UserWarning: WARNING: API returned no data for 245 dates. These dates will NOT have any records uploaded.\n",
      "Sample dates: [datetime.date(2025, 1, 1), datetime.date(2025, 1, 2), datetime.date(2025, 1, 3), datetime.date(2025, 1, 4), datetime.date(2025, 1, 5), datetime.date(2025, 1, 6), datetime.date(2025, 1, 7), datetime.date(2025, 1, 8), datetime.date(2025, 1, 9), datetime.date(2025, 1, 10)]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing final deduplication check before upload...\n",
      "No duplicates found in final data (count: 59540)\n",
      "Uploading to database...\n",
      "Target timezone for ops.isone_hourly_energy: America/New_York\n",
      "Only datetime_he column was provided. datetime_hb column was created...\n",
      "Beginning data upload of 59540 rows in update mode...\n",
      "Data uploaded to database. Beginning merge with updates...\n",
      "Update complete! Updated/inserted 59540 rows into ops.isone_hourly_energy\n",
      "Upload complete!\n"
     ]
    }
   ],
   "source": [
    "# Automatically refresh data in energy positions database\n",
    "db_table_name = \"ops.isone_hourly_energy\"\n",
    "mis_report = \"SR_RTLOCSUM\"\n",
    "automated_isone_data_update(USERNAME, token, db_table_name, tz, mis_report, datetime(2025,1,1), fill_with_zeros=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab17fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target timezone for ops.backup_isone_hourly_ancillary_2026_01_06: America/New_York\n",
      "Successfully processed both datetime_he and datetime_hb columns...\n",
      "Beginning upload to create a new table or overwrite an existing one...\n",
      "Created new table ops.backup_isone_hourly_ancillary_2026_01_06 with 1629960 rows!\n"
     ]
    }
   ],
   "source": [
    "# # Set parameters for database query\n",
    "# USERNAME = r\"firstlightpower\\cbrooks\"\n",
    "# db_table_name = \"ops.isone_hourly_energy\"\n",
    "\n",
    "# # Query existing data in database\n",
    "# db_conn = flp_database_connector(USERNAME)\n",
    "# sql_query = f\"\"\"\n",
    "#     SELECT *\n",
    "#     FROM {db_table_name}\n",
    "# \"\"\"\n",
    "# existing_data = db_conn.read_from_db(\"DataQuant01\", \"\", sql_query)\n",
    "\n",
    "# db_conn.upload_data_to_quant_db(\n",
    "#                     table_name=\"ops.backup_isone_hourly_ancillary_2026_01_06\",\n",
    "#                     df=existing_data,\n",
    "#                     tz='America/New_York',\n",
    "#                     mode=\"create\",\n",
    "#                     skip_prompt=True\n",
    "#                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e805aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    " # Set parameters for database query & upload\n",
    "USERNAME = r\"firstlightpower\\cbrooks\"\n",
    "db_table_name = \"ops.isone_hourly_ancillary\"\n",
    "tz = 'America/New_York'\n",
    "\n",
    "# Query existing data in database\n",
    "db_conn = flp_database_connector(USERNAME)\n",
    "sql_query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {db_table_name}\n",
    "\"\"\"\n",
    "existing_data = db_conn.read_from_db(\"DataQuant01\", \"\", sql_query)\n",
    "\n",
    "start_date = existing_data['datetime_he'].min().date()\n",
    "        \n",
    "# End date: end of day 2 days prior to today\n",
    "end_date = (datetime.now().date() - timedelta(days=2))\n",
    "\n",
    "# Create expected datetime range (hourly intervals)\n",
    "expected_datetimes = pd.date_range(\n",
    "    start=start_date,\n",
    "    end=end_date + timedelta(days=1),  # Include full last day\n",
    "    freq='h',\n",
    "    tz=tz\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a93a642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for missing data from 2025-03-01 to 2025-11-24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cbrooks\\AppData\\Local\\Temp\\ipykernel_32712\\2554004317.py:16: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  expected_datetimes = pd.date_range(\n"
     ]
    }
   ],
   "source": [
    "# Convert to UTC first, then convert to target timezone\n",
    "existing_data['datetime_he'] = pd.to_datetime(\n",
    "    existing_data['datetime_he'], \n",
    "    utc=True\n",
    ").dt.tz_convert('America/New_York')\n",
    "\n",
    "# Start date: earliest date in existing data\n",
    "start_date = existing_data['datetime_he'].min().date()\n",
    "\n",
    "# End date: end of day 2 days prior to today\n",
    "end_date = (datetime.now().date() - timedelta(days=2))\n",
    "\n",
    "print(f\"Checking for missing data from {start_date} to {end_date}\")\n",
    "\n",
    "# Create expected datetime range (hourly intervals) - timezone-aware\n",
    "expected_datetimes = pd.date_range(\n",
    "    start=start_date,\n",
    "    end=end_date + timedelta(days=1),  # Include full last day\n",
    "    freq='H',\n",
    "    tz='America/New_York'  # Make timezone-aware\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ead035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 40)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(range(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9daf0160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import warnings\n",
    "import re\n",
    "from zoneinfo import ZoneInfo   # Python 3.9+; or use pytz if you prefer\n",
    "from pandas.api.types import is_datetime64_any_dtype, is_string_dtype, infer_dtype\n",
    "from pytz.exceptions import AmbiguousTimeError, NonExistentTimeError\n",
    "\n",
    "TZ = \"America/New_York\"\n",
    "\n",
    "check = pd.Series([datetime.datetime(2025,11,26,12, tzinfo=ZoneInfo(TZ)), datetime.datetime(2025,11,26,13, tzinfo=ZoneInfo(TZ))])\n",
    "\n",
    "tests = {\n",
    "    \"naive_strings\": {\n",
    "        \"input\": pd.Series([\"2025-11-26 12:00\", \"2025-11-26 13:00\"]),\n",
    "        \"expect\": \"ok\"   # localize to TZ, preserve wall-clock times\n",
    "    },\n",
    "    \"naive_datetimes\": {\n",
    "        \"input\": pd.Series([datetime.datetime(2025,11,26,12), datetime.datetime(2025,11,26,13)]),\n",
    "        \"expect\": \"ok\"\n",
    "    },\n",
    "    \"tzaware_matching_timestamp\": {\n",
    "        \"input\":pd.Series([pd.Timestamp(\"2025-11-26 12:00\", tz=TZ), pd.Timestamp(\"2025-11-26 13:00\", tz=TZ)]),\n",
    "        \"expect\": \"ok\"  # accept as-is because tz matches\n",
    "    },\n",
    "    \"tzaware_matching_datetime_zoneinfo\": {\n",
    "        \"input\":pd.Series([datetime.datetime(2025,11,26,12, tzinfo=ZoneInfo(TZ)), datetime.datetime(2025,11,26,13, tzinfo=ZoneInfo(TZ))]),\n",
    "        \"expect\": \"ok\"\n",
    "    },\n",
    "    \"tzaware_different_tz\": {\n",
    "        \"input\": pd.Series([pd.Timestamp(\"2025-11-26 12:00\", tz=\"UTC\"), pd.Timestamp(\"2025-11-26 13:00\", tz=\"UTC\")]),\n",
    "        \"expect\": \"raise\"\n",
    "    },\n",
    "    \"string_offset_matching\": {\n",
    "        # On 2025-11-26 New_York is UTC-05:00 (EST). This string uses -05:00 so should match.\n",
    "        \"input\": pd.Series([\"2025-11-26T12:00:00-05:00\", \"2025-11-26T13:00:00-05:00\"]),\n",
    "        \"expect\": \"ok\"\n",
    "    },\n",
    "    \"string_offset_not_matching\": {\n",
    "        \"input\": pd.Series([\"2025-11-26T18:00:00+01:00\", \"2025-11-26T19:00:00+01:00\"]),\n",
    "        \"expect\": \"ok\" # This is a different timezone than the target timezone, but the input is explicit about what it should be so assume user is doing it intentionally; warn and convert\n",
    "    },\n",
    "    \"mixed_string_and_dt\": {\n",
    "        \"input\": pd.Series([\"2025-11-26 12:00\", datetime.datetime(2025,11,26,13)]),\n",
    "        \"expect\": \"raise\"\n",
    "    },\n",
    "    \"mixed_naive_and_tzaware\": {\n",
    "        \"input\": pd.Series([datetime.datetime(2025,11,26,12), pd.Timestamp(\"2025-11-26 13:00\", tz=TZ)]),\n",
    "        \"expect\": \"raise\"\n",
    "    },\n",
    "    \"empty_series\": {\n",
    "        \"input\": pd.Series([]),\n",
    "        \"expect\": \"raise\"\n",
    "    },\n",
    "    \"invalid_strings\": {\n",
    "        \"input\": pd.Series([\"not a date\", \"2025-11-26 13:00\"]),\n",
    "        \"expect\": \"raise\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "300b471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_datetime(col, tz):\n",
    "    \"\"\"\n",
    "    Vectorized datetime handling:\n",
    "    \n",
    "    Rules:\n",
    "    - Mixtures of types or naive/tz-aware -> raise.\n",
    "    - TZ-aware datetime -> must match tz, else raise.\n",
    "    - Naive datetime -> localize to tz (warn once).\n",
    "    - Strings with offsets -> must match tz at that datetime, else raise.\n",
    "    - Strings without offsets -> localize to tz (warn).\n",
    "    \n",
    "    Returns: pd.Series of tz-aware datetimes in tz.\n",
    "    \"\"\"\n",
    "    # First check for mixed data types\n",
    "    inferred_type = infer_dtype(col)\n",
    "    if \"mixed\" in inferred_type:\n",
    "        raise ValueError(\"Ambiguous input: mixture of text and datetime objects.\")\n",
    "\n",
    "    # Next, check if col is already a datetime\n",
    "    if is_datetime64_any_dtype(col):\n",
    "        # Check if it's timezone-aware\n",
    "        if col.dt.tz is not None:\n",
    "            # Check if it matches the target timezone\n",
    "            if str(col.dt.tz) == str(tz):\n",
    "                return col\n",
    "            else:\n",
    "                raise ValueError(f\"Datetime column has timezone {col.dt.tz} but expected {tz}\")\n",
    "        else:\n",
    "            # Localize to tz\n",
    "            warnings.warn(\n",
    "                \"Datetime input has no explicit timezone; assuming it's in \"\n",
    "                f\"'{tz}' and localizing accordingly.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            return col.dt.tz_localize(tz)\n",
    "    elif is_string_dtype(col):\n",
    "        _OFFSET_RE = re.compile(r'(?:Z|[+-]\\d{2}:?\\d{2})$')\n",
    "\n",
    "        # Check for mixed naive/aware using string patterns\n",
    "        str_view = col.astype(\"string\").fillna(\"\")\n",
    "        has_offset = str_view.str.match(r'.*' + _OFFSET_RE.pattern)\n",
    "        \n",
    "        if has_offset.any() and not has_offset.all():\n",
    "            raise ValueError(\n",
    "                f\"Datetime input contains a mix of offset-aware and naive strings. \"\n",
    "                \"All timestamps must be consistently naive or aware.\"\n",
    "            )\n",
    "\n",
    "        # Parse the strings to datetime objects\n",
    "        dt_series = pd.to_datetime(col)\n",
    "        \n",
    "        # Check if the series has timezone info\n",
    "        if dt_series.dt.tz is None:\n",
    "            # Naive datetime - localize to target timezone (assume already in that timezone)\n",
    "            try:\n",
    "                result = dt_series.dt.tz_localize(tz, ambiguous='infer')\n",
    "            except AmbiguousTimeError as e:\n",
    "                raise ValueError(\n",
    "                    f\"Ambiguous times detected in datetime input that cannot be inferred. \"\n",
    "                    f\"This typically means the data is not sorted chronologically. \"\n",
    "                    f\"Original error: {e}\"\n",
    "                )\n",
    "            except NonExistentTimeError as e:\n",
    "                raise ValueError(\n",
    "                    f\"Non-existent times detected in datetime input. \"\n",
    "                    f\"This occurs during 'spring forward' DST transitions when an hour is skipped. \"\n",
    "                    f\"Original error: {e}\"\n",
    "                )\n",
    "            \n",
    "            warnings.warn(\n",
    "                \"Datetime input has no explicit timezone; assuming it's in \"\n",
    "                f\"'{tz}' and localizing accordingly.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            return result\n",
    "        else:\n",
    "            # All aware - check for mixed timezones (excluding DST variations)\n",
    "            tz_objects = dt_series.apply(lambda x: str(x.tzinfo) if x.tzinfo else None)\n",
    "            unique_tz_objects = tz_objects.unique()\n",
    "            \n",
    "            if len(unique_tz_objects) > 1:\n",
    "                raise ValueError(\n",
    "                    f\"Mixed timezones detected in datetime input: {list(unique_tz_objects)}. \"\n",
    "                    f\"All timestamps must be in the same timezone.\"\n",
    "                )\n",
    "            \n",
    "            # Convert to target timezone\n",
    "            print(f\"Converting from UTC offset {dt_series.dt.tz} to {tz}\")\n",
    "            return dt_series.dt.tz_convert(tz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd5d2526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive_strings passed, output successful.\n",
      "\n",
      "\n",
      "naive_datetimes passed, output successful.\n",
      "\n",
      "\n",
      "tzaware_matching_timestamp passed, output successful.\n",
      "\n",
      "\n",
      "tzaware_matching_datetime_zoneinfo passed, output successful.\n",
      "\n",
      "\n",
      "tzaware_different_tz passed, error raised as expected: Datetime column has timezone UTC but expected America/New_York\n",
      "\n",
      "\n",
      "Converting from UTC offset UTC-05:00 to America/New_York\n",
      "string_offset_matching passed, output successful.\n",
      "\n",
      "\n",
      "Converting from UTC offset UTC+01:00 to America/New_York\n",
      "string_offset_not_matching passed, output successful.\n",
      "\n",
      "\n",
      "mixed_string_and_dt passed, error raised as expected: Ambiguous input: mixture of text and datetime objects.\n",
      "\n",
      "\n",
      "mixed_naive_and_tzaware failed - epected error, but got output that doesn't match target.\n",
      "\n",
      "\n",
      "empty_series failed - epected error, but got output that doesn't match target.\n",
      "\n",
      "\n",
      "invalid_strings passed, error raised as expected: Unknown datetime string format, unable to parse: not a date, at position 0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cbrooks\\AppData\\Local\\Temp\\ipykernel_19272\\1267081005.py:70: UserWarning: Datetime input has no explicit timezone; assuming it's in 'America/New_York' and localizing accordingly.\n",
      "  warnings.warn(\n",
      "C:\\Users\\cbrooks\\AppData\\Local\\Temp\\ipykernel_19272\\1267081005.py:30: UserWarning: Datetime input has no explicit timezone; assuming it's in 'America/New_York' and localizing accordingly.\n",
      "  warnings.warn(\n",
      "C:\\Users\\cbrooks\\AppData\\Local\\Temp\\ipykernel_19272\\1267081005.py:70: UserWarning: Datetime input has no explicit timezone; assuming it's in 'America/New_York' and localizing accordingly.\n",
      "  warnings.warn(\n",
      "C:\\Users\\cbrooks\\AppData\\Local\\Temp\\ipykernel_19272\\1267081005.py:50: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  dt_series = pd.to_datetime(col)\n"
     ]
    }
   ],
   "source": [
    "from pandas.testing import assert_series_equal\n",
    "\n",
    "def series_equal(s1, s2):\n",
    "    \"\"\"Check if two series are equal, handling timezone differences.\"\"\"\n",
    "    try:\n",
    "        # Check if both are datetime with timezones\n",
    "        if hasattr(s1.dtype, 'tz') and hasattr(s2.dtype, 'tz'):\n",
    "            # Convert both to UTC to normalize timezone objects\n",
    "            assert_series_equal(s1.dt.tz_convert('UTC'), s2.dt.tz_convert('UTC'))\n",
    "        else:\n",
    "            assert_series_equal(s1, s2)\n",
    "        return True\n",
    "    except (AssertionError, Exception):\n",
    "        return False\n",
    "\n",
    "for test_name, test_data in tests.items():\n",
    "    try:\n",
    "        result = _ensure_datetime(test_data[\"input\"], TZ)\n",
    "        if test_data[\"expect\"] == \"ok\" and series_equal(result,check):\n",
    "            print(f\"{test_name} passed, output successful.\")\n",
    "        elif test_data[\"expect\"] == \"raise\" and series_equal(result,check):\n",
    "            print(f\"{test_name} failed - epected error, but output matches target.\")\n",
    "        elif test_data[\"expect\"] == \"ok\":\n",
    "            print(f\"{test_name} failed - output does not match target.\\nOutput: {result}\\nExpected: {check}\")\n",
    "        elif test_data[\"expect\"] == \"raise\":\n",
    "            print(f\"{test_name} failed - epected error, but got output that doesn't match target.\")\n",
    "        else:\n",
    "            print(\"Shouldn't be able to get here!\")\n",
    "    except ValueError as e:\n",
    "        if test_data[\"expect\"] == \"ok\":\n",
    "            print(f\"{test_name} failed - expected success but got error: {e}\")\n",
    "        elif test_data[\"expect\"] == \"raise\":\n",
    "            print(f\"{test_name} passed, error raised as expected: {e}\")\n",
    "        else:\n",
    "            print(\"Shouldn't be able to get here!\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e0a5afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pharos_ams_query import query_ams_with_basic_auth\n",
    "from src.process_as_positions import process_daas_cleared_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50280c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 duplicate rows removed based on most recent Version.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime_he</th>\n",
       "      <th>asset</th>\n",
       "      <th>name</th>\n",
       "      <th>ops_type</th>\n",
       "      <th>service</th>\n",
       "      <th>da_volume</th>\n",
       "      <th>rt_volume</th>\n",
       "      <th>unit</th>\n",
       "      <th>interval_width_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>2025-03-01 01:00:00-05:00</td>\n",
       "      <td>Northfield Mountain</td>\n",
       "      <td>NORTHFIELD MOUNTAIN 1</td>\n",
       "      <td>Generation</td>\n",
       "      <td>EIR</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>MW</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>2025-03-01 01:00:00-05:00</td>\n",
       "      <td>Northfield Mountain</td>\n",
       "      <td>NORTHFIELD MOUNTAIN 2</td>\n",
       "      <td>Generation</td>\n",
       "      <td>EIR</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>MW</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>2025-03-01 01:00:00-05:00</td>\n",
       "      <td>Northfield Mountain</td>\n",
       "      <td>NORTHFIELD MOUNTAIN 3</td>\n",
       "      <td>Generation</td>\n",
       "      <td>EIR</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>MW</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>2025-03-01 01:00:00-05:00</td>\n",
       "      <td>Northfield Mountain</td>\n",
       "      <td>NORTHFIELD MOUNTAIN 4</td>\n",
       "      <td>Generation</td>\n",
       "      <td>EIR</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>MW</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>2025-03-01 01:00:00-05:00</td>\n",
       "      <td>Northfield Mountain</td>\n",
       "      <td>NORTHFIELD MOUNTAIN 1</td>\n",
       "      <td>Generation</td>\n",
       "      <td>TMNSR</td>\n",
       "      <td>265</td>\n",
       "      <td></td>\n",
       "      <td>MW</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>2025-03-03 00:00:00-05:00</td>\n",
       "      <td>Northfield Mountain</td>\n",
       "      <td>NORTHFIELD MOUNTAIN PUMP 3</td>\n",
       "      <td>Pumping</td>\n",
       "      <td>TMNSR</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>MW</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>2025-03-03 00:00:00-05:00</td>\n",
       "      <td>Northfield Mountain</td>\n",
       "      <td>NORTHFIELD MOUNTAIN 1</td>\n",
       "      <td>Generation</td>\n",
       "      <td>TMOR</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>MW</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>2025-03-03 00:00:00-05:00</td>\n",
       "      <td>Northfield Mountain</td>\n",
       "      <td>NORTHFIELD MOUNTAIN PUMP 3</td>\n",
       "      <td>Pumping</td>\n",
       "      <td>TMOR</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>MW</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2025-03-03 00:00:00-05:00</td>\n",
       "      <td>Northfield Mountain</td>\n",
       "      <td>NORTHFIELD MOUNTAIN 1</td>\n",
       "      <td>Generation</td>\n",
       "      <td>TMSR</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>MW</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2025-03-03 00:00:00-05:00</td>\n",
       "      <td>Northfield Mountain</td>\n",
       "      <td>NORTHFIELD MOUNTAIN PUMP 3</td>\n",
       "      <td>Pumping</td>\n",
       "      <td>TMSR</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>MW</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>572 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  datetime_he                asset  \\\n",
       "429 2025-03-01 01:00:00-05:00  Northfield Mountain   \n",
       "430 2025-03-01 01:00:00-05:00  Northfield Mountain   \n",
       "431 2025-03-01 01:00:00-05:00  Northfield Mountain   \n",
       "432 2025-03-01 01:00:00-05:00  Northfield Mountain   \n",
       "143 2025-03-01 01:00:00-05:00  Northfield Mountain   \n",
       "..                        ...                  ...   \n",
       "285 2025-03-03 00:00:00-05:00  Northfield Mountain   \n",
       "427 2025-03-03 00:00:00-05:00  Northfield Mountain   \n",
       "428 2025-03-03 00:00:00-05:00  Northfield Mountain   \n",
       "141 2025-03-03 00:00:00-05:00  Northfield Mountain   \n",
       "142 2025-03-03 00:00:00-05:00  Northfield Mountain   \n",
       "\n",
       "                           name    ops_type service da_volume rt_volume unit  \\\n",
       "429       NORTHFIELD MOUNTAIN 1  Generation     EIR         0             MW   \n",
       "430       NORTHFIELD MOUNTAIN 2  Generation     EIR         0             MW   \n",
       "431       NORTHFIELD MOUNTAIN 3  Generation     EIR         0             MW   \n",
       "432       NORTHFIELD MOUNTAIN 4  Generation     EIR         0             MW   \n",
       "143       NORTHFIELD MOUNTAIN 1  Generation   TMNSR       265             MW   \n",
       "..                          ...         ...     ...       ...       ...  ...   \n",
       "285  NORTHFIELD MOUNTAIN PUMP 3     Pumping   TMNSR         0             MW   \n",
       "427       NORTHFIELD MOUNTAIN 1  Generation    TMOR         0             MW   \n",
       "428  NORTHFIELD MOUNTAIN PUMP 3     Pumping    TMOR         0             MW   \n",
       "141       NORTHFIELD MOUNTAIN 1  Generation    TMSR         0             MW   \n",
       "142  NORTHFIELD MOUNTAIN PUMP 3     Pumping    TMSR         0             MW   \n",
       "\n",
       "     interval_width_s  \n",
       "429              3600  \n",
       "430              3600  \n",
       "431              3600  \n",
       "432              3600  \n",
       "143              3600  \n",
       "..                ...  \n",
       "285              3600  \n",
       "427              3600  \n",
       "428              3600  \n",
       "141              3600  \n",
       "142              3600  \n",
       "\n",
       "[572 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = keyring.get_password(\"pharos_api\", \"api_token\")\n",
    "group_start = '2025-03-01'\n",
    "group_end = '2025-03-02'\n",
    "most_recent_version = 'true'\n",
    "mis_report = 'SD_DAASCLEARED'\n",
    "url = f\"https://ams.pharos-ei.com/api/v2/isone/mis/downloads.csv?organization_key=ho-fl&settle_since={group_start}&settle_before={group_end}&most_recent_version={most_recent_version}&report_name={mis_report}\"\n",
    "                \n",
    "df_raw = query_ams_with_basic_auth(url, token)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "data_folder = os.path.join(project_root, \"data\")\n",
    "mapping_path = os.path.join(data_folder,\"maps\",\"ISONE Location Mapping.csv\")\n",
    "# Parse raw MIS data\n",
    "df_final = process_daas_cleared_data([df_raw], mapping_path)\n",
    "df_final\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
